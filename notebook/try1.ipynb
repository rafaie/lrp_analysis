{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load MNL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_matched.tsv\n",
      "dev_mismatched.tsv\n",
      "diagnostic.tsv\n",
      "diagnostic-full.tsv\n",
      "original\n",
      "README.txt\n",
      "test_matched.tsv\n",
      "test_mismatched.tsv\n",
      "train.tsv\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9816,\n",
       " ['index',\n",
       "  'promptID',\n",
       "  'pairID',\n",
       "  'genre',\n",
       "  'sentence1_binary_parse',\n",
       "  'sentence2_binary_parse',\n",
       "  'sentence1_parse',\n",
       "  'sentence2_parse',\n",
       "  'sentence1',\n",
       "  'sentence2',\n",
       "  'label1',\n",
       "  'label2',\n",
       "  'label3',\n",
       "  'label4',\n",
       "  'label5',\n",
       "  'gold_label'])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched = []\n",
    "with open('../data/MNLI/dev_matched.tsv', 'r', encoding=\"utf8\") as fi:\n",
    "    for l in fi:\n",
    "        matched.append(l.replace(\"\\n\", \"\").split('\\t'))\n",
    "\n",
    "len(matched), matched[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9833,\n",
       " ['index',\n",
       "  'promptID',\n",
       "  'pairID',\n",
       "  'genre',\n",
       "  'sentence1_binary_parse',\n",
       "  'sentence2_binary_parse',\n",
       "  'sentence1_parse',\n",
       "  'sentence2_parse',\n",
       "  'sentence1',\n",
       "  'sentence2',\n",
       "  'label1',\n",
       "  'label2',\n",
       "  'label3',\n",
       "  'label4',\n",
       "  'label5',\n",
       "  'gold_label'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatched = []\n",
    "with open('../data/MNLI/dev_mismatched.tsv', 'r', encoding=\"utf8\") as fi:\n",
    "    for l in fi:\n",
    "        mismatched.append(l.replace(\"\\n\", \"\").split('\\t'))\n",
    "\n",
    "len(mismatched), mismatched[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load LRP data and merge with MNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list.pkl\n",
      "data_list-new.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19647,\n",
       " dict_keys(['final_prob', 'answer', 'prediction', 'text_lst', 'text', 'question', 'sentence', 'answer_list', 'reward0', 'reward_norm0', 'reward_norm_dict0', 'reward1', 'reward_norm1', 'reward_norm_dict1', 'reward2', 'reward_norm2', 'reward_norm_dict2']))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open('data_list-new.pkl', 'rb'))\n",
    "len(data), data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Merge with MNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = matched[0]\n",
    "for i, m in enumerate(matched[1:]):\n",
    "    d = {}\n",
    "    for ii, l in enumerate(lbl):\n",
    "        d[l] = m[ii]\n",
    "    if len(data) <= i:\n",
    "        break\n",
    "    data[i]['src'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = mismatched[0]\n",
    "for i, m in enumerate(mismatched[1:]):\n",
    "    d = {}\n",
    "    for ii, l in enumerate(lbl):\n",
    "        d[l] = m[ii]\n",
    "    if len(data) <= i:\n",
    "        break\n",
    "    data[i + 9815]['src'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'index': '9831',\n",
       "  'promptID': '8693',\n",
       "  'pairID': '8693e',\n",
       "  'genre': 'verbatim',\n",
       "  'sentence1_binary_parse': \"( ( ( ( ( ( ( ( ( ( ( Bloomer ( -LRB- ( ( for ( ` ( flower ' ) ) ) -RRB- ) ) ) , ) ( butter ( -LRB- ( ( for ( ` ( ram ' ) ) ) -RRB- ) ) ) ) , ) or ) ( even flower ) ) ( -LRB- ( ( for ( ` ( river ' ) ) ) -RRB- ) ) ) ( are ( recurrent examples ) ) ) , ) but ) ( solvers ( ( must always ) ( be ( on ( ( the alert ) ( for ( ( new traps ) ( of this ) ) ) ) ) ) ) ) )\",\n",
       "  'sentence2_binary_parse': '( ( Bloomer ( is ( ( another word ) ( for flower ) ) ) ) ( , ( butter ( ( is ( for ( ( ( ram and ) flower ) ( for river ) ) ) ) . ) ) ) )',\n",
       "  'sentence1_parse': \"(ROOT (FRAG (S (S (NP (NP (NP (NP (NNP Bloomer)) (PRN (-LRB- -LRB-) (PP (IN for) (NP (`` `) (NN flower) ('' '))) (-RRB- -RRB-))) (, ,) (NP (NP (NN butter)) (PRN (-LRB- -LRB-) (PP (IN for) (NP (`` `) (NN ram) ('' '))) (-RRB- -RRB-))) (, ,) (CC or) (NP (RB even) (NN flower))) (PRN (-LRB- -LRB-) (PP (IN for) (NP (`` `) (NN river) ('' '))) (-RRB- -RRB-))) (VP (VBP are) (NP (JJ recurrent) (NNS examples)))) (, ,) (CC but) (S (NP (NNS solvers)) (VP (MD must) (ADVP (RB always)) (VP (VB be) (PP (IN on) (NP (NP (DT the) (NN alert)) (PP (IN for) (NP (NP (JJ new) (NNS traps)) (PP (IN of) (NP (DT this)))))))))))))\",\n",
       "  'sentence2_parse': '(ROOT (S (S (NP (NNP Bloomer)) (VP (VBZ is) (NP (NP (DT another) (NN word)) (PP (IN for) (NP (NN flower)))))) (, ,) (NP (NN butter)) (VP (VBZ is) (PP (IN for) (NP (NP (NN ram) (CC and) (NN flower)) (PP (IN for) (NP (NN river)))))) (. .)))',\n",
       "  'sentence1': \"Bloomer (for `flower'), butter (for `ram'), or even flower (for `river') are recurrent examples, but solvers must always be on the alert for new traps of this \",\n",
       "  'sentence2': 'Bloomer is another word for flower, butter is for ram and flower for river. ',\n",
       "  'label1': 'entailment',\n",
       "  'label2': 'entailment',\n",
       "  'label3': 'entailment',\n",
       "  'label4': 'entailment',\n",
       "  'label5': 'entailment',\n",
       "  'gold_label': 'entailment'},\n",
       " '||||',\n",
       " \"bloom ##er ( for ` flower ' ) , butter ( for ` ram ' ) , or even flower ( for ` river ' ) are rec ##urrent examples , but solve ##rs must always be on the alert for new traps of this\",\n",
       " 'bloom ##er is another word for flower , butter is for ram and flower for river .',\n",
       " 65,\n",
       " dict_keys(['final_prob', 'answer', 'prediction', 'text_lst', 'text', 'question', 'sentence', 'answer_list', 'reward0', 'reward_norm0', 'reward_norm_dict0', 'reward1', 'reward_norm1', 'reward_norm_dict1', 'reward2', 'reward_norm2', 'reward_norm_dict2', 'src']))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = -1\n",
    "data[i]['src'], '||||', data[i]['question'], data[i]['sentence'], len(data[i]['reward0'][0]), data[i].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Reconnect splitted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space_to_punc(txt):\n",
    "    punc = './:{}()'\n",
    "    for p in punc:\n",
    "        txt = txt.replace(p, ' ' + p + ' ')\n",
    "    txt = txt.replace(\"'s\", \" 's \").replace(\"n't\", \" n't \")\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19647/19647 [00:01<00:00, 19588.71it/s]\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "for i in tqdm.tqdm(range(len(data))):\n",
    "    question = add_space_to_punc(data[i]['src']['sentence1']).split()\n",
    "    sentence = add_space_to_punc(data[i]['src']['sentence2']).split()\n",
    "    question_id = []\n",
    "    sentence_id = []\n",
    "    cnt = 0\n",
    "    is_q = True\n",
    "    ii = -1\n",
    "\n",
    "    while ii < len(data[i]['text_lst']) - 1:\n",
    "        ii += 1\n",
    "        w = data[i]['text_lst'][ii]\n",
    "        if w == '[CLS]':\n",
    "            src = question\n",
    "            target = question_id\n",
    "            cnt = 0\n",
    "            continue\n",
    "\n",
    "        if debug is True:\n",
    "            print(w, ii)\n",
    "\n",
    "        if w == '[SEP]':\n",
    "            is_q = False\n",
    "            src = sentence\n",
    "            target = sentence_id\n",
    "\n",
    "            if debug is True:\n",
    "                print('-' * 70)\n",
    "\n",
    "            cnt = 0\n",
    "        elif w == src[cnt].lower().strip():\n",
    "            if debug is True:\n",
    "                target.append([ii, w])\n",
    "            else:\n",
    "                target.append([ii])\n",
    "\n",
    "            cnt +=1\n",
    "        elif w == src[cnt].lower().strip()[:len(w)]:\n",
    "            if debug is True:\n",
    "                print(w, src[cnt])\n",
    "\n",
    "            target.append([])\n",
    "            for iii in range(1, 8):\n",
    "                w_new = ''.join(data[i]['text_lst'][ii:ii+iii]).replace(\"#\", \"\")\n",
    "                if w_new != src[cnt].lower().strip()[:len(w_new)]:\n",
    "                    ii += (iii - 2)\n",
    "                    cnt += 1\n",
    "                    if debug is True:\n",
    "                        print(ii, cnt)\n",
    "                        if ii < len(data[i]['text_lst']) and cnt < len(src):\n",
    "                            print(\"===>\", w_new, data[i]['text_lst'][ii] , src[cnt].lower().strip())\n",
    "\n",
    "                    break\n",
    "                else:\n",
    "                    if debug is True:\n",
    "                        target[-1] += [ii + iii - 1, w_new]\n",
    "                    else:\n",
    "                        target[-1] += [ii + iii - 1]\n",
    "    \n",
    "    data[i]['question_ids'] = question_id\n",
    "    data[i]['sentence_ids'] = sentence_id\n",
    "    data[i]['question_list'] = question\n",
    "    data[i]['sentence_list'] = sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19647/19647 [00:04<00:00, 4042.55it/s]\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "for i in tqdm.tqdm(range(len(data))):\n",
    "#     print(i)\n",
    "    for j in range(2):\n",
    "        if j == 0:\n",
    "            txt = data[i]['src']['sentence1_parse']\n",
    "            txt_list = data[i]['question_list']\n",
    "            data[i]['question_pos'] = []\n",
    "            target = data[i]['question_pos']\n",
    "        else:\n",
    "            txt = data[i]['src']['sentence2_parse']\n",
    "            txt_list = data[i]['answer_list']\n",
    "            data[i]['answer_pos'] = []\n",
    "            target = data[i]['answer_pos']\n",
    "\n",
    "        pos_list = []\n",
    "        st = ''\n",
    "        for t in txt:\n",
    "            if t == '(':\n",
    "                st = ''\n",
    "            elif t == ')':\n",
    "                if len(st) > 0:\n",
    "                    st_1 =  re.sub(\"[^0-9a-z]\", \"\", st.lower().split()[-1])\n",
    "                    if st[0] != '-' and len(st_1) > 0:\n",
    "                        pos_list.append(st)\n",
    "                st = ''\n",
    "            else:\n",
    "                st += t\n",
    "        pos_list2 = [p.split() for p in pos_list]\n",
    "        sentence1_pos = ['']* len(txt_list)\n",
    "\n",
    "        iii = 0\n",
    "        for ii, w in enumerate(txt_list):\n",
    "            w_new =  re.sub(\"[^0-9a-z]\", \"\", w.lower())\n",
    "            if debug is True:\n",
    "                print(ii, iii, w, w_new)\n",
    "\n",
    "            if len(w_new) > 0:\n",
    "                pos_w = re.sub(\"[^0-9a-z]\", \"\", pos_list2[iii][-1].lower())\n",
    "                if debug is True:\n",
    "                    print('===>', w_new, pos_w)\n",
    "\n",
    "                if w_new == pos_w:\n",
    "                    sentence1_pos[ii] = pos_list2[iii][0]\n",
    "                    iii += 1\n",
    "        target = sentence1_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Recalculate rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19647/19647 [00:01<00:00, 19148.96it/s]\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "for i in tqdm.tqdm(range(len(data))):\n",
    "    if debug is True:\n",
    "        print(i)\n",
    "\n",
    "    data[i]['question_id_reward0'] = []\n",
    "    data[i]['question_id_reward1'] = []\n",
    "    data[i]['question_id_reward2'] = []\n",
    "    \n",
    "    for l1 in data[i]['question_ids']:\n",
    "        data[i]['question_id_reward0'].append(sum([data[i]['reward0'][0][l] for l in l1]))\n",
    "        data[i]['question_id_reward1'].append(sum([data[i]['reward1'][0][l] for l in l1]))\n",
    "        data[i]['question_id_reward2'].append(sum([data[i]['reward2'][0][l] for l in l1]))\n",
    "\n",
    "    data[i]['sentence_id_reward0'] = []\n",
    "    data[i]['sentence_id_reward1'] = []\n",
    "    data[i]['sentence_id_reward2'] = []\n",
    "    \n",
    "    for l1 in data[i]['sentence_ids']:\n",
    "        data[i]['sentence_id_reward0'].append(sum([data[i]['reward0'][0][l] for l in l1]))\n",
    "        data[i]['sentence_id_reward1'].append(sum([data[i]['reward1'][0][l] for l in l1]))\n",
    "        data[i]['sentence_id_reward2'].append(sum([data[i]['reward2'][0][l] for l in l1]))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.002134892623871565,\n",
       "  -0.0014312209095805883,\n",
       "  -0.0038709824439138174,\n",
       "  -0.009584102779626846,\n",
       "  0.002653901930898428,\n",
       "  -0.002251827740110457,\n",
       "  0.00963689386844635,\n",
       "  -0.0017088635358959436,\n",
       "  -0.00034058024175465107,\n",
       "  -0.01138190645724535,\n",
       "  -0.0016721236752346158,\n",
       "  -0.0015907816123217344,\n",
       "  -0.000903072883374989,\n",
       "  -0.00022454402642324567,\n",
       "  0.005671633407473564],\n",
       " [0.02040218375623226,\n",
       "  -0.0010138035286217928,\n",
       "  0.0014954593498259783,\n",
       "  0.0030733866151422262,\n",
       "  -0.0005306612583808601,\n",
       "  0.006268484517931938,\n",
       "  -0.0027839052490890026,\n",
       "  -0.00012536458962131292,\n",
       "  -0.0043652961030602455,\n",
       "  0.0018189530819654465,\n",
       "  0.0050583165138959885,\n",
       "  0.0031399717554450035,\n",
       "  -0.00022344780154526234,\n",
       "  0.002818953013047576,\n",
       "  0.0021987920626997948,\n",
       "  -0.0008798292838037014,\n",
       "  -0.0014544443693012,\n",
       "  0.0034465391654521227,\n",
       "  -0.0006571013946086168,\n",
       "  -0.004202545620501041,\n",
       "  -0.002362002618610859,\n",
       "  0.0037485056091099977,\n",
       "  -0.0020993894431740046,\n",
       "  0.001844980288296938,\n",
       "  -0.002464176854118705,\n",
       "  -0.0017556531820446253,\n",
       "  2.998911077156663e-05,\n",
       "  -0.00023667566711083055,\n",
       "  0.004900538828223944,\n",
       "  0.003668807679787278,\n",
       "  0.004857772961258888,\n",
       "  0.0015314084012061357,\n",
       "  -0.004167092498391867,\n",
       "  0.00527661107480526,\n",
       "  -0.0010820666793733835,\n",
       "  0.0007390343816950917,\n",
       "  0.0005092096398584545,\n",
       "  0.0008171062218025327,\n",
       "  -0.0014446277637034655,\n",
       "  -0.0010226150043308735,\n",
       "  -0.005621697753667831,\n",
       "  -0.0026817647740244865,\n",
       "  -0.0007365758065134287,\n",
       "  -0.006110175047069788,\n",
       "  -0.0002788978163152933,\n",
       "  0.0011149966157972813,\n",
       "  -0.0018178666941821575,\n",
       "  -0.00012724846601486206,\n",
       "  0.002262141089886427,\n",
       "  -0.0014312209095805883,\n",
       "  -0.0038709824439138174,\n",
       "  -0.009584102779626846,\n",
       "  0.002653901930898428,\n",
       "  0.001052071456797421,\n",
       "  -0.003303899196907878,\n",
       "  0.00963689386844635,\n",
       "  -0.0017088635358959436,\n",
       "  -0.00034058024175465107,\n",
       "  -0.01138190645724535,\n",
       "  -0.0016721236752346158,\n",
       "  -0.0015907816123217344,\n",
       "  -0.000903072883374989,\n",
       "  -0.00022454402642324567,\n",
       "  0.005671633407473564,\n",
       "  0.002342475578188896])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i]['sentence_id_reward0'], data[i]['reward0'][0]\n",
    "#, data[i]['sentence_id_reward1'], data[i]['sentence_id_reward2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] 0.0004816558212041855\n",
      "[3] 0.0030733866151422262\n",
      "[4] -0.0005306612583808601\n",
      "[5, 6, 7] 0.0033592146792216226\n",
      "[8] -0.0043652961030602455\n",
      "[9] 0.0018189530819654465\n",
      "[10] 0.0050583165138959885\n",
      "[11] 0.0031399717554450035\n",
      "[12] -0.00022344780154526234\n",
      "[13, 14, 15] 0.004137915791943669\n",
      "[16] -0.0014544443693012\n",
      "[17] 0.0034465391654521227\n",
      "[18] -0.0006571013946086168\n",
      "[19] -0.004202545620501041\n",
      "[20] -0.002362002618610859\n",
      "[21] 0.0037485056091099977\n",
      "[22] -0.0020993894431740046\n",
      "[23, 24, 25] -0.002374849747866392\n",
      "[26] 2.998911077156663e-05\n",
      "[27] -0.00023667566711083055\n",
      "[28, 29] 0.008569346508011222\n",
      "[30, 31] 0.006389181362465024\n",
      "[32] -0.004167092498391867\n",
      "[33, 34] 0.004194544395431876\n",
      "[35] 0.0007390343816950917\n",
      "[36] 0.0005092096398584545\n",
      "[37] 0.0008171062218025327\n",
      "[38] -0.0014446277637034655\n",
      "[39] -0.0010226150043308735\n",
      "[40] -0.005621697753667831\n",
      "[41] -0.0026817647740244865\n",
      "[42] -0.0007365758065134287\n",
      "[43] -0.006110175047069788\n",
      "[44] -0.0002788978163152933\n",
      "[45] 0.0011149966157972813\n"
     ]
    }
   ],
   "source": [
    "for l1 in data[i]['question_ids']:\n",
    "    print(l1, sum([ data[i]['reward0'][0][l] for l in l1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Candidates candidates\n",
      "===> candidates candidates\n",
      "1 1 must must\n",
      "===> must must\n",
      "2 2 submit submit\n",
      "===> submit submit\n",
      "3 3 a a\n",
      "===> a a\n",
      "4 4 set set\n",
      "===> set set\n",
      "5 5 of of\n",
      "===> of of\n",
      "6 6 fingerprints fingerprints\n",
      "===> fingerprints fingerprints\n",
      "7 7 for for\n",
      "===> for for\n",
      "8 8 review review\n",
      "===> review review\n",
      "9 9 by by\n",
      "===> by by\n",
      "10 10 the the\n",
      "===> the the\n",
      "11 11 FBI fbi\n",
      "===> fbi fbi\n",
      "12 12 . \n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "i = 122\n",
    "j = 0\n",
    "if j == 0:\n",
    "    txt = data[i]['src']['sentence1_parse']\n",
    "    txt_list = data[i]['question_list']\n",
    "    data[i]['question_pos'] = []\n",
    "    target = data[i]['question_pos']\n",
    "else:\n",
    "    txt = data[i]['src']['sentence2_parse']\n",
    "    txt_list = data[i]['answer_list']\n",
    "    data[i]['answer_pos'] = []\n",
    "    target = data[i]['answer_pos']\n",
    "\n",
    "pos_list = []\n",
    "st = ''\n",
    "for t in txt:\n",
    "    if t == '(':\n",
    "        st = ''\n",
    "    elif t == ')':\n",
    "        if len(st) > 0:\n",
    "            st_1 =  re.sub(\"[^0-9a-z]\", \"\", st.lower().split()[-1])\n",
    "            if st[0] != '-' and len(st_1) > 0:\n",
    "                pos_list.append(st)\n",
    "        st = ''\n",
    "    else:\n",
    "        st += t\n",
    "pos_list2 = [p.split() for p in pos_list]\n",
    "sentence1_pos = ['']* len(txt_list)\n",
    "\n",
    "iii = 0\n",
    "for ii, w in enumerate(txt_list):\n",
    "    w_new =  re.sub(\"[^0-9a-z]\", \"\", w.lower())\n",
    "    if debug is True:\n",
    "        print(ii, iii, w, w_new)\n",
    "\n",
    "    if len(w_new) > 0:\n",
    "        pos_w = re.sub(\"[^0-9a-z]\", \"\", pos_list2[iii][-1].lower())\n",
    "        if debug is True:\n",
    "            print('===>', w_new, pos_w)\n",
    "\n",
    "        if w_new == pos_w:\n",
    "            sentence1_pos[ii] = pos_list2[iii][0]\n",
    "            iii += 1\n",
    "target = sentence1_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19647"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,\n",
       " \"uh i don ' t know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him i like him for the most part , but would still enjoy seeing someone beat him .\",\n",
       " 'i like him for the most part , but would still enjoy seeing someone beat him .',\n",
       " 50)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[i]['sentence'].split()), data[i]['text'],data[i]['sentence'], len(data[i]['text_lst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1\n",
      "- --the\n",
      "3 1\n",
      "===> --theteachers the teachers\n",
      "teachers 4\n",
      "were 5\n",
      "much 6\n",
      "more 7\n",
      "more more,\n",
      "8 5\n",
      "===> more,i , i\n",
      "i 9\n",
      "guess 10\n",
      "guess guess,\n",
      "11 7\n",
      "===> guess,i , i\n",
      "i 12\n",
      "don 13\n",
      "don don't\n",
      "15 9\n",
      "===> don'tknow t know???\n",
      "know 16\n",
      "know know???\n",
      "19 10\n",
      "[SEP] 20\n",
      "----------------------------------------------------------------------\n",
      "i 21\n",
      "don 22\n",
      "don don't\n",
      "24 2\n",
      "===> don'tknow t know,\n",
      "know 25\n",
      "know know,\n",
      "26 3\n",
      "===> know,the , the\n",
      "the 27\n",
      "teachers 28\n",
      "were 29\n",
      "- 30\n",
      "- ---??\n",
      "34 7\n",
      "[SEP] 35\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[1, '-', 2, '--', 3, '--the'],\n",
       "  [4, 'teachers'],\n",
       "  [5, 'were'],\n",
       "  [6, 'much'],\n",
       "  [7, 'more', 8, 'more,'],\n",
       "  [9, 'i'],\n",
       "  [10, 'guess', 11, 'guess,'],\n",
       "  [12, 'i'],\n",
       "  [13, 'don', 14, \"don'\", 15, \"don't\"],\n",
       "  [16, 'know', 17, 'know?', 18, 'know??', 19, 'know???']],\n",
       " [[21, 'i'],\n",
       "  [22, 'don', 23, \"don'\", 24, \"don't\"],\n",
       "  [25, 'know', 26, 'know,'],\n",
       "  [27, 'the'],\n",
       "  [28, 'teachers'],\n",
       "  [29, 'were'],\n",
       "  [30, '-', 31, '--', 32, '---', 33, '---?', 34, '---??']],\n",
       " '||||',\n",
       " ['--the',\n",
       "  'teachers',\n",
       "  'were',\n",
       "  'much',\n",
       "  'more,',\n",
       "  'I',\n",
       "  'guess,',\n",
       "  'I',\n",
       "  \"don't\",\n",
       "  'know???'],\n",
       " ['I', \"don't\", 'know,', 'the', 'teachers', 'were', '---??'],\n",
       " '|||',\n",
       " {'index': '5080',\n",
       "  'promptID': '38542',\n",
       "  'pairID': '38542e',\n",
       "  'genre': 'facetoface',\n",
       "  'sentence1_binary_parse': \"( -- ( ( the teachers ) ( were ( ( ( ( much more ) ( , ( I guess ) ) ) , ) ( I ( ( do n't ) ( know ??? ) ) ) ) ) ) )\",\n",
       "  'sentence2_binary_parse': \"( I ( ( do n't ) ( ( ( ( know , ) ( ( the teachers ) were ) ) -- ) ?? ) ) )\",\n",
       "  'sentence1_parse': \"(ROOT (PRN (: --) (S (NP (DT the) (NNS teachers)) (VP (VBD were) (ADJP (ADJP (RB much) (JJR more)) (PRN (, ,) (S (NP (PRP I)) (VP (VBP guess)))) (, ,) (SBAR (S (NP (PRP I)) (VP (VBP do) (RB n't) (VP (VB know) (NP (NNS ???)))))))))))\",\n",
       "  'sentence2_parse': \"(ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (VP (VP (VB know) (, ,) (SBAR (S (NP (DT the) (NNS teachers)) (VP (VBD were))))) (: --) (VP (VB ??))))))\",\n",
       "  'sentence1': \"--the teachers were much more, I guess, I don't know???\",\n",
       "  'sentence2': \"I don't know, the teachers were ---??\",\n",
       "  'label1': 'entailment',\n",
       "  'label2': 'entailment',\n",
       "  'label3': 'entailment',\n",
       "  'label4': 'entailment',\n",
       "  'label5': 'entailment',\n",
       "  'gold_label': 'entailment'},\n",
       " ['[CLS]',\n",
       "  '-',\n",
       "  '-',\n",
       "  'the',\n",
       "  'teachers',\n",
       "  'were',\n",
       "  'much',\n",
       "  'more',\n",
       "  ',',\n",
       "  'i',\n",
       "  'guess',\n",
       "  ',',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'know',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?',\n",
       "  '[SEP]',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'know',\n",
       "  ',',\n",
       "  'the',\n",
       "  'teachers',\n",
       "  'were',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '?',\n",
       "  '?',\n",
       "  '[SEP]'])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 14895\n",
    "debug = True\n",
    "\n",
    "question = add_space_to_punc(data[i]['src']['sentence1']).split()\n",
    "sentence = add_space_to_punc(data[i]['src']['sentence2']).split()\n",
    "question_id = []\n",
    "sentence_id = []\n",
    "cnt = 0\n",
    "is_q = True\n",
    "ii = -1\n",
    "\n",
    "while ii < len(data[i]['text_lst']) - 1:\n",
    "    ii += 1\n",
    "    w = data[i]['text_lst'][ii]\n",
    "    if w == '[CLS]':\n",
    "        src = question\n",
    "        target = question_id\n",
    "        cnt = 0\n",
    "        continue\n",
    "\n",
    "    if debug is True:\n",
    "        print(w, ii)\n",
    "\n",
    "    if w == '[SEP]':\n",
    "        is_q = False\n",
    "        src = sentence\n",
    "        target = sentence_id\n",
    "        \n",
    "        if debug is True:\n",
    "            print('-' * 70)\n",
    "        \n",
    "        cnt = 0\n",
    "    elif w == src[cnt].lower().strip():\n",
    "        if debug is True:\n",
    "            target.append([ii, w])\n",
    "        else:\n",
    "            target.append([ii])\n",
    "\n",
    "        cnt +=1\n",
    "    elif w == src[cnt].lower().strip()[:len(w)]:\n",
    "        if debug is True:\n",
    "            print(w, src[cnt])\n",
    "            \n",
    "        target.append([])\n",
    "        for iii in range(1, 8):\n",
    "            w_new = ''.join(data[i]['text_lst'][ii:ii+iii]).replace(\"#\", \"\")\n",
    "            if w_new != src[cnt].lower().strip()[:len(w_new)]:\n",
    "                ii += (iii - 2)\n",
    "                cnt += 1\n",
    "                if debug is True:\n",
    "                    print(ii, cnt)\n",
    "                    if ii < len(data[i]['text_lst']) and cnt < len(src):\n",
    "                        print(\"===>\", w_new, data[i]['text_lst'][ii] , src[cnt].lower().strip())\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                if debug is True:\n",
    "                    target[-1] += [ii + iii - 1, w_new]\n",
    "                else:\n",
    "                    target[-1] += [ii + iii - 1]\n",
    "                    \n",
    "question_id, sentence_id, '||||', question, sentence, '|||', data[i]['src'], data[i]['text_lst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, '-', 2, '--', 3, '--the'],\n",
       "  [4, 'teachers'],\n",
       "  [5, 'were'],\n",
       "  [6, 'much'],\n",
       "  [7, 'more', 8, 'more,'],\n",
       "  [9, 'i'],\n",
       "  [10, 'guess', 11, 'guess,'],\n",
       "  [12, 'i'],\n",
       "  [13, 'don', 14, \"don'\", 15, \"don't\"],\n",
       "  [16, 'know', 17, 'know?', 18, 'know??', 19, 'know???']],\n",
       " [[21, 'i'],\n",
       "  [22, 'don', 23, \"don'\", 24, \"don't\"],\n",
       "  [25, 'know', 26, 'know,'],\n",
       "  [27, 'the'],\n",
       "  [28, 'teachers'],\n",
       "  [29, 'were'],\n",
       "  [30, '-', 31, '--', 32, '---', 33, '---?', 34, '---??']],\n",
       " 36,\n",
       " '||||',\n",
       " ['--the',\n",
       "  'teachers',\n",
       "  'were',\n",
       "  'much',\n",
       "  'more,',\n",
       "  'I',\n",
       "  'guess,',\n",
       "  'I',\n",
       "  \"don't\",\n",
       "  'know???'],\n",
       " ['I', \"don't\", 'know,', 'the', 'teachers', 'were', '---??'],\n",
       " '|||',\n",
       " ['[CLS]',\n",
       "  '-',\n",
       "  '-',\n",
       "  'the',\n",
       "  'teachers',\n",
       "  'were',\n",
       "  'much',\n",
       "  'more',\n",
       "  ',',\n",
       "  'i',\n",
       "  'guess',\n",
       "  ',',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'know',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?',\n",
       "  '[SEP]',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'know',\n",
       "  ',',\n",
       "  'the',\n",
       "  'teachers',\n",
       "  'were',\n",
       "  '-',\n",
       "  '-',\n",
       "  '-',\n",
       "  '?',\n",
       "  '?',\n",
       "  '[SEP]'],\n",
       " {'index': '5080',\n",
       "  'promptID': '38542',\n",
       "  'pairID': '38542e',\n",
       "  'genre': 'facetoface',\n",
       "  'sentence1_binary_parse': \"( -- ( ( the teachers ) ( were ( ( ( ( much more ) ( , ( I guess ) ) ) , ) ( I ( ( do n't ) ( know ??? ) ) ) ) ) ) )\",\n",
       "  'sentence2_binary_parse': \"( I ( ( do n't ) ( ( ( ( know , ) ( ( the teachers ) were ) ) -- ) ?? ) ) )\",\n",
       "  'sentence1_parse': \"(ROOT (PRN (: --) (S (NP (DT the) (NNS teachers)) (VP (VBD were) (ADJP (ADJP (RB much) (JJR more)) (PRN (, ,) (S (NP (PRP I)) (VP (VBP guess)))) (, ,) (SBAR (S (NP (PRP I)) (VP (VBP do) (RB n't) (VP (VB know) (NP (NNS ???)))))))))))\",\n",
       "  'sentence2_parse': \"(ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (VP (VP (VB know) (, ,) (SBAR (S (NP (DT the) (NNS teachers)) (VP (VBD were))))) (: --) (VP (VB ??))))))\",\n",
       "  'sentence1': \"--the teachers were much more, I guess, I don't know???\",\n",
       "  'sentence2': \"I don't know, the teachers were ---??\",\n",
       "  'label1': 'entailment',\n",
       "  'label2': 'entailment',\n",
       "  'label3': 'entailment',\n",
       "  'label4': 'entailment',\n",
       "  'label5': 'entailment',\n",
       "  'gold_label': 'entailment'})"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_id, sentence_id, len(data[i]['text_lst']), '||||', question, sentence, '|||', data[i]['text_lst'], data[i]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'site',\n",
       " 'includes',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'all',\n",
       " 'award',\n",
       " 'winners',\n",
       " 'and',\n",
       " 'a',\n",
       " 'search',\n",
       " '##able',\n",
       " 'database',\n",
       " 'of',\n",
       " 'government',\n",
       " 'executive',\n",
       " 'articles',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'the',\n",
       " 'government',\n",
       " 'executive',\n",
       " 'articles',\n",
       " 'housed',\n",
       " 'on',\n",
       " 'the',\n",
       " 'website',\n",
       " 'are',\n",
       " 'not',\n",
       " 'able',\n",
       " 'to',\n",
       " 'be',\n",
       " 'searched',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i]['text_lst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['final_prob', 'answer', 'prediction', 'text_lst', 'text', 'question', 'sentence', 'answer_list', 'reward0', 'reward_norm0', 'reward_norm_dict0', 'reward1', 'reward_norm1', 'reward_norm_dict1', 'reward2', 'reward_norm2', 'reward_norm_dict2', 'src', 'question_ids', 'sentence_ids', 'question_list', 'sentence_list', 'question_pos', 'answer_pos', 'question_id_reward0', 'question_id_reward1', 'question_id_reward2', 'sentence_id_reward0', 'sentence_id_reward1', 'sentence_id_reward2'])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0037531480193138123,\n",
       " 0.006491946056485176,\n",
       " 0.00605211453512311,\n",
       " 0.0017841905355453491,\n",
       " 0.0005510298069566488,\n",
       " 0.007249016081914306,\n",
       " -0.01052998099476099,\n",
       " 0.008899006992578506,\n",
       " 0.012983223423361778,\n",
       " 0.0051713059656322,\n",
       " -0.009235817939043045,\n",
       " -0.004486431367695332,\n",
       " -0.008614360354840755,\n",
       " -0.015913477167487144,\n",
       " -0.006748727988451719]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i]['sentence_id_reward1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
